{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Canada': {'2010': 47560.6666009406, '2011': 52223.8588398531, '2012': 52670.3447335415, '2013': 52638.1187235237, '2014': 50960.8431174661, '2015': 43594.1941045394, '2016': 42314.0615817218, '2017': 45129.628116623, '2018': 46539.1761570405, '2019': 46352.8693445211, '2020': 43537.839298904, '2021': 52496.8441693242, '2022': 55509.393176404, '2023': 53431.1857063879}, 'Finland': {'2010': 46506.2919016566, '2011': 51060.3242589767, '2012': 47551.6740841369, '2013': 49691.0145200739, '2014': 50073.7760815871, '2015': 42560.3456767103, '2016': 43451.2562442158, '2017': 46085.0174739036, '2018': 49654.2497035329, '2019': 48358.1807773701, '2020': 48828.6846862799, '2021': 53099.1351400148, '2022': 50438.4753952355, '2023': 52925.6897638424}, 'Italy': {'2010': 36184.7118698678, '2011': 38851.3881339353, '2012': 35235.7989058242, '2013': 35747.707952689, '2014': 35750.7197500382, '2015': 30387.1293187854, '2016': 31126.3246947273, '2017': 32554.14668453, '2018': 34746.3441392416, '2019': 33812.788724868, '2020': 32091.4866621366, '2021': 36852.5425414951, '2022': 35635.7442204098, '2023': 39003.3160954123}, 'Kenya': {'2010': 1091.51873101194, '2011': 1096.14499807675, '2012': 1285.00536171512, '2013': 1370.87885214659, '2014': 1482.81566253497, '2015': 1489.11959777284, '2016': 1554.12610314423, '2017': 1667.48447230835, '2018': 1836.45275528993, '2019': 1960.40808854893, '2020': 1927.66459027849, '2021': 2061.35622089594, '2022': 2109.56288513137, '2023': 1952.30457929636}, 'Norway': {'2010': 88163.2085931423, '2011': 101221.813476644, '2012': 102175.919298374, '2013': 103553.840134417, '2014': 97666.6951838749, '2015': 74809.9658049898, '2016': 70867.3609970749, '2017': 76131.8384032764, '2018': 82792.8427113304, '2019': 76430.5889473338, '2020': 68340.0181033702, '2021': 93072.8925119571, '2022': 108798.451165901, '2023': 87925.0944188399}, 'Singapore': {'2010': 47236.683084953, '2011': 53891.4570264372, '2012': 55547.5553077786, '2013': 56967.4257940383, '2014': 57564.8023114977, '2015': 55645.6068614606, '2016': 56899.9181805173, '2017': 61162.0973932771, '2018': 66840.6373389791, '2019': 66081.7199235165, '2020': 61466.803676358, '2021': 79601.4129622433, '2022': 88428.7024226232, '2023': 84734.2559206054}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Specify the countries and indicator\n",
    "countries = \"CAN;FIN;ITA;KEN;NOR;SGP\"  # ISO codes for Canada, Finland, Italy, Kenya, Norway, Singapore\n",
    "indicator = \"NY.GDP.PCAP.CD\"  # GDP per capita (current US$)\n",
    "years = [\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\",\"2021\",\"2022\",\"2023\"]\n",
    "\n",
    "gdp_data = {}\n",
    "\n",
    "for year in years:\n",
    "        \n",
    "    url = f\"https://api.worldbank.org/v2/country/{countries}/indicator/{indicator}?format=json&date={year}\"\n",
    "    # Fetch the data\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract relevant data\n",
    "    for entry in data[1]:\n",
    "        country = entry['country']['value']\n",
    "        gdp_per_capita = entry['value']\n",
    "        year = entry['date']\n",
    "        if country not in gdp_data:\n",
    "            gdp_data[country] = {}\n",
    "            gdp_data[country][year] = gdp_per_capita\n",
    "        else:\n",
    "            if year not in gdp_data[country]:\n",
    "                gdp_data[country][year] = gdp_per_capita\n",
    "\n",
    "\n",
    "print(gdp_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Canada' 'Finland' 'Italy' 'Kenya' 'Norway' 'Singapore']\n",
      "Train data\n",
      "  country              store             product  num_sold  is_weekend  \\\n",
      "1  Canada  Discount Stickers              Kaggle     973.0       False   \n",
      "2  Canada  Discount Stickers        Kaggle Tiers     906.0       False   \n",
      "3  Canada  Discount Stickers            Kerneler     423.0       False   \n",
      "4  Canada  Discount Stickers  Kerneler Dark Mode     491.0       False   \n",
      "5  Canada  Stickers for Less   Holographic Goose     300.0       False   \n",
      "\n",
      "   sin_dayofweek  cos_dayofweek  sin_month  cos_month  sin_year  cos_year  \\\n",
      "1      -0.433884      -0.900969        0.0        1.0       0.0       1.0   \n",
      "2      -0.433884      -0.900969        0.0        1.0       0.0       1.0   \n",
      "3      -0.433884      -0.900969        0.0        1.0       0.0       1.0   \n",
      "4      -0.433884      -0.900969        0.0        1.0       0.0       1.0   \n",
      "5      -0.433884      -0.900969        0.0        1.0       0.0       1.0   \n",
      "\n",
      "   gdp_per_capita  \n",
      "1    47560.666601  \n",
      "2    47560.666601  \n",
      "3    47560.666601  \n",
      "4    47560.666601  \n",
      "5    47560.666601  \n",
      "Test data\n",
      "       id country              store             product  is_weekend  \\\n",
      "0  230130  Canada  Discount Stickers   Holographic Goose        True   \n",
      "1  230131  Canada  Discount Stickers              Kaggle        True   \n",
      "2  230132  Canada  Discount Stickers        Kaggle Tiers        True   \n",
      "3  230133  Canada  Discount Stickers            Kerneler        True   \n",
      "4  230134  Canada  Discount Stickers  Kerneler Dark Mode        True   \n",
      "\n",
      "   sin_dayofweek  cos_dayofweek  sin_month  cos_month  sin_year  cos_year  \\\n",
      "0      -0.781831        0.62349        0.0        1.0 -0.951057 -0.309017   \n",
      "1      -0.781831        0.62349        0.0        1.0 -0.951057 -0.309017   \n",
      "2      -0.781831        0.62349        0.0        1.0 -0.951057 -0.309017   \n",
      "3      -0.781831        0.62349        0.0        1.0 -0.951057 -0.309017   \n",
      "4      -0.781831        0.62349        0.0        1.0 -0.951057 -0.309017   \n",
      "\n",
      "   gdp_per_capita  \n",
      "0    45129.628117  \n",
      "1    45129.628117  \n",
      "2    45129.628117  \n",
      "3    45129.628117  \n",
      "4    45129.628117  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "def compute_gdp_per_capita(X,gdp_data):\n",
    "    def get_gdp(row):\n",
    "        country = str(row[\"country\"])\n",
    "        year = str(int(row[\"year\"])+2010)\n",
    "        return gdp_data[country][year]\n",
    "    X[\"gdp_per_capita\"] = X.apply(get_gdp, axis=1)\n",
    "    return X\n",
    "\n",
    "def generate_features(X, train=False):\n",
    "    X = X.copy()\n",
    "        \n",
    "    if train:\n",
    "        X = X.dropna(subset=[\"num_sold\"])\n",
    "        X = X.drop(columns=[\"id\"])\n",
    "\n",
    "    X[\"date\"] = pd.to_datetime(X[\"date\"], format=\"%Y-%m-%d\")\n",
    "    X[\"dayofweek\"] = X[\"date\"].dt.dayofweek\n",
    "    X[\"month\"] = X[\"date\"].dt.month - 1\n",
    "    X[\"year\"] = X[\"date\"].dt.year - 2010\n",
    "    X[\"is_weekend\"] = X[\"dayofweek\"].isin([5,6])\n",
    "    X[\"sin_dayofweek\"] = np.sin(X[\"dayofweek\"] * (2 * np.pi / 7))\n",
    "    X[\"cos_dayofweek\"] = np.cos(X[\"dayofweek\"] * (2 * np.pi / 7))\n",
    "    X[\"sin_month\"] = np.sin(X[\"month\"] * (2 * np.pi / 12))\n",
    "    X[\"cos_month\"] = np.cos(X[\"month\"] * (2 * np.pi / 12))\n",
    "    X[\"sin_year\"] = np.sin(X[\"year\"] * (2 * np.pi / 10))\n",
    "    X[\"cos_year\"] = np.cos(X[\"year\"] * (2 * np.pi / 10))\n",
    "    \n",
    "    X = compute_gdp_per_capita(X,gdp_data)\n",
    "    \n",
    "    X = X.drop(columns=[\"date\",\"month\",\"year\",\"dayofweek\"])\n",
    "        \n",
    "    return X\n",
    "\n",
    "FOLDER = \"playground-series-s5e1/\"\n",
    "train_data = pd.read_csv(FOLDER + \"train.csv\")\n",
    "test_data = pd.read_csv(FOLDER + \"test.csv\")\n",
    "\n",
    "print(train_data[\"country\"].unique())\n",
    "train_data = generate_features(train_data,train=True)\n",
    "test_data = generate_features(test_data)\n",
    "\n",
    "print(\"Train data\")\n",
    "print(train_data.head())\n",
    "print(\"Test data\")\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 15:06:38,556] A new study created in memory with name: no-name-2d2e70d2-d55d-4152-a26e-e14082ce8e22\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X = train_data.drop(columns=['num_sold'])\n",
    "y = train_data['num_sold']\n",
    "\n",
    "cat_cols = ['country', 'store', 'product']\n",
    "num_cols = [\"sin_dayofweek\", \"cos_dayofweek\", \"sin_month\", \"cos_month\", \"sin_year\", \"cos_year\",\"gdp_per_capita\"]\n",
    "\n",
    "# Create different preprocessors for different models\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# For models that need one-hot encoding (XGBoost, RandomForest)\n",
    "categorical_transformer_ohe = Pipeline(steps=[\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create different preprocessors\n",
    "preprocessor_ohe = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, num_cols),\n",
    "    ('cat', categorical_transformer_ohe, cat_cols)\n",
    "])\n",
    "\n",
    "# For LightGBM and CatBoost, we'll just scale numerics and pass categoricals as is\n",
    "preprocessor_native = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, num_cols)\n",
    "])\n",
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_ohe),\n",
    "    ('regressor', XGBRegressor(\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    #params max_depth, min_child_weight, gamma, learning_rate\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 10)\n",
    "    gamma = trial.suggest_int(\"gamma\", 0, 10)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 5000)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.5, 1)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1)\n",
    "    colsample_bylevel = trial.suggest_float(\"colsample_bylevel\", 0.5, 1)\n",
    "    colsample_bynode = trial.suggest_float(\"colsample_bynode\", 0.5, 1)\n",
    "    lambda_l1 = trial.suggest_float(\"lambda_l1\", 0, 10)\n",
    "    lambda_l2 = trial.suggest_float(\"lambda_l2\", 0, 10)\n",
    "    \n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor_ohe),\n",
    "        ('regressor', XGBRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            min_child_weight=min_child_weight,\n",
    "            gamma=gamma,\n",
    "            random_state=42,\n",
    "            objective='reg:squarederror',\n",
    "            n_jobs=4,\n",
    "            booster=\"gbtree\",\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            colsample_bylevel=colsample_bylevel,\n",
    "            colsample_bynode=colsample_bynode,\n",
    "            reg_alpha=lambda_l1,\n",
    "            reg_lambda=lambda_l2\n",
    "        ))\n",
    "    ])\n",
    "    #Time-series cross-validation\n",
    "    time_series = TimeSeriesSplit(n_splits=5)\n",
    "    fold_scores_val= []\n",
    "    fold_scores_train= []\n",
    "    loss_train= []\n",
    "    loss_val= []\n",
    "    model_name = \"xgboost\"\n",
    "    for fold_index, (train_index, val_index) in enumerate(time_series.split(X)):\n",
    "        X_train_fold = X.iloc[train_index]\n",
    "        y_train_fold = y.iloc[train_index]\n",
    "        X_val_fold = X.iloc[val_index]\n",
    "        y_val_fold = y.iloc[val_index]\n",
    "        \n",
    "        model_pipeline.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_val = model_pipeline.predict(X_val_fold)    \n",
    "        y_pred_train = model_pipeline.predict(X_train_fold)\n",
    "        \n",
    "        mape_val = mean_absolute_percentage_error(y_val_fold, y_pred_val)\n",
    "        mape_train = mean_absolute_percentage_error(y_train_fold, y_pred_train)\n",
    "        \n",
    "        mse_train = mean_squared_error(y_train_fold, y_pred_train)\n",
    "        mse_val = mean_squared_error(y_val_fold, y_pred_val)\n",
    "        \n",
    "        fold_scores_val.append(mape_val)\n",
    "        fold_scores_train.append(mape_train)\n",
    "        loss_train.append(mse_train)\n",
    "        loss_val.append(mse_val)\n",
    "        \n",
    "        # print(f\"Fold {fold_index + 1} loss train: {mse_train:.4f}\")\n",
    "        # print(f\"Fold {fold_index + 1} loss val: {mse_val:.4f}\")\n",
    "        # print(\"--------------------------------\")\n",
    "        # print(f\"Fold {fold_index + 1} metric val: {mape_val:.4f}\")\n",
    "        # print(f\"Fold {fold_index + 1} metric train: {mape_train:.4f}\")\n",
    "        # print(\"--------------------------------\")\n",
    "        \n",
    "    print(f\"Average MAPE for {model_name}: {np.mean(fold_scores_val):.4f}\")\n",
    "    print(f\"Average MAPE for {model_name} train: {np.mean(fold_scores_train):.4f}\")\n",
    "    # print(f\"Average MSE for {model_name} train: {np.mean(loss_train):.4f}\")\n",
    "    # print(f\"Average MSE for {model_name} val: {np.mean(loss_val):.4f}\")\n",
    "    \n",
    "    return np.mean(fold_scores_val)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=40)\n",
    "print(f\"Best parameters: {study.best_params}\")\n",
    "print(f\"Best value: {study.best_value:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
